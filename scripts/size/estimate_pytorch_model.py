# -*- coding: utf-8 -*-
"""estimate_pytorch_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gY9QZx-N7GVZwl0hUad62H_oDlYcnmqx
"""

# -*- coding: utf-8 -*-
import torch
import torchvision.models as models

# Lista de modelos a analizar
model_configs = [
    ("squeezenet1_1", models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)),
    ("vgg16", models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)),
    ("efficientnet_v2_l", models.efficientnet_v2_l(pretrained=True)),
    ("mobilenet_v2", models.mobilenet_v2(pretrained=True))
]

# Archivo de salida
output_file = "resultados_size.txt"

with open(output_file, "w", encoding="utf-8") as f:
    for model_name, model in model_configs:
        # Mover el modelo a la GPU si está disponible
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        # Contar el número total de parámetros
        total_params = sum(p.numel() for p in model.parameters())

        # Estimar el uso de memoria en FP32 (4 bytes por parámetro)
        memory_bytes = total_params * 4
        memory_mb = memory_bytes / (1024 ** 2)
        memory_gb = memory_bytes / (1024 ** 3)

        # Estimar el uso de memoria en FP16 (2 bytes por parámetro)
        memory_bytes_fp16 = total_params * 2
        memory_mb_fp16 = memory_bytes_fp16 / (1024 ** 2)
        memory_gb_fp16 = memory_bytes_fp16 / (1024 ** 3)

        # Escribir los resultados en el archivo
        f.write(f"Modelo: {model_name}\n")
        f.write(f"Número total de parámetros: {total_params:,}\n")
        f.write(f"Uso de memoria (FP32): {memory_mb:.2f} MB ({memory_gb:.2f} GB)\n")
        f.write(f"Uso de memoria (FP16): {memory_mb_fp16:.2f} MB ({memory_gb_fp16:.2f} GB)\n")
        f.write("-" * 40 + "\n")

print(f"Resultados guardados en {output_file}")